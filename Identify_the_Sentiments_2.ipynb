{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Identify the Sentiments_DL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQd2XyOF2JaP",
        "outputId": "69c4ccaa-5b16-4177-c6b3-62b57dc376fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget --header=\"Host: datahack-prod.s3.amazonaws.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-IN,en-US;q=0.9,en-GB;q=0.8,en;q=0.7,te;q=0.6\" --header=\"Referer: https://datahack.analyticsvidhya.com/\" \"https://datahack-prod.s3.amazonaws.com/train_file/train_2kmZucJ.csv\" -c -O 'train_2kmZucJ.csv'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-20 08:20:53--  https://datahack-prod.s3.amazonaws.com/train_file/train_2kmZucJ.csv\n",
            "Resolving datahack-prod.s3.amazonaws.com (datahack-prod.s3.amazonaws.com)... 52.219.64.28\n",
            "Connecting to datahack-prod.s3.amazonaws.com (datahack-prod.s3.amazonaws.com)|52.219.64.28|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1100229 (1.0M) [application/vnd.ms-excel]\n",
            "Saving to: ‘train_2kmZucJ.csv’\n",
            "\n",
            "train_2kmZucJ.csv   100%[===================>]   1.05M  1.61MB/s    in 0.7s    \n",
            "\n",
            "2020-10-20 08:20:54 (1.61 MB/s) - ‘train_2kmZucJ.csv’ saved [1100229/1100229]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvIxrSxd2TQM",
        "outputId": "8c853ac3-ded8-49a8-83ef-bfc79a965844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget --header=\"Host: datahack-prod.s3.amazonaws.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-IN,en-US;q=0.9,en-GB;q=0.8,en;q=0.7,te;q=0.6\" --header=\"Referer: https://datahack.analyticsvidhya.com/\" \"https://datahack-prod.s3.amazonaws.com/test_file/test_oJQbWVk.csv\" -c -O 'test_oJQbWVk.csv'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-20 08:20:55--  https://datahack-prod.s3.amazonaws.com/test_file/test_oJQbWVk.csv\n",
            "Resolving datahack-prod.s3.amazonaws.com (datahack-prod.s3.amazonaws.com)... 52.219.64.64\n",
            "Connecting to datahack-prod.s3.amazonaws.com (datahack-prod.s3.amazonaws.com)|52.219.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 263010 (257K) [application/vnd.ms-excel]\n",
            "Saving to: ‘test_oJQbWVk.csv’\n",
            "\n",
            "test_oJQbWVk.csv    100%[===================>] 256.85K   634KB/s    in 0.4s    \n",
            "\n",
            "2020-10-20 08:20:55 (634 KB/s) - ‘test_oJQbWVk.csv’ saved [263010/263010]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C_2sn2a2Vug",
        "outputId": "4050c246-6209-402b-c225-9ae53bfc7188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!git clone 'https://github.com/google-research/bert'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'bert'...\n",
            "remote: Enumerating objects: 340, done.\u001b[K\n",
            "remote: Total 340 (delta 0), reused 0 (delta 0), pack-reused 340\u001b[K\n",
            "Receiving objects: 100% (340/340), 317.85 KiB | 545.00 KiB/s, done.\n",
            "Resolving deltas: 100% (185/185), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdprMBPL2Ygb",
        "outputId": "f0b205b0-71b6-4b50-fd0e-7c529a86dcbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget --header=\"Host: storage.googleapis.com\" --header=\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-IN,en-US;q=0.9,en-GB;q=0.8,en;q=0.7,te;q=0.6\" --header=\"Referer: https://github.com/google-research/bert\" \"https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\" -c -O 'uncased_L-12_H-768_A-12.zip'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-20 08:20:59--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.188.128, 64.233.189.128, 108.177.97.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.188.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407727028 (389M) [application/zip]\n",
            "Saving to: ‘uncased_L-12_H-768_A-12.zip’\n",
            "\n",
            "uncased_L-12_H-768_ 100%[===================>] 388.84M  80.1MB/s    in 4.9s    \n",
            "\n",
            "2020-10-20 08:21:04 (80.1 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sKvzmIR2ao5"
      },
      "source": [
        "import shutil\n",
        "#shutil.unpack_archive(\"glove.840B.300d.txt.zip\")\n",
        "shutil.unpack_archive('uncased_L-12_H-768_A-12.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T29R3daz2gzA"
      },
      "source": [
        "#1. Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8iA_SkT2dcB",
        "outputId": "6cc7c9dc-5cde-467d-ab04-685471c10b7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import subprocess\n",
        "from tqdm import tqdm,tqdm_notebook\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfkcGoao2fbo",
        "outputId": "9f25883a-e2a0-4411-e84f-beab3d6437fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "!pip install bert-serving-server\n",
        "!pip install bert-serving-client"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-serving-server\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/bd/cab677bbd0c5fb08b72e468371d2bca6ed9507785739b4656b0b5470d90b/bert_serving_server-1.10.0-py3-none-any.whl (61kB)\n",
            "\r\u001b[K     |█████▎                          | 10kB 17.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (1.1.0)\n",
            "Collecting GPUtil>=1.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (1.18.5)\n",
            "Requirement already satisfied: pyzmq>=17.1.0 in /usr/local/lib/python3.6/dist-packages (from bert-serving-server) (19.0.2)\n",
            "Building wheels for collected packages: GPUtil\n",
            "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for GPUtil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7411 sha256=a84f1de96d14bd7d8d72cebdb21e9f7ca26516157fd5347490bf9154281084dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built GPUtil\n",
            "Installing collected packages: GPUtil, bert-serving-server\n",
            "Successfully installed GPUtil-1.4.0 bert-serving-server-1.10.0\n",
            "Collecting bert-serving-client\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/09/aae1405378a848b2e87769ad89a43d6d71978c4e15534ca48e82e723a72f/bert_serving_client-1.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pyzmq>=17.1.0 in /usr/local/lib/python3.6/dist-packages (from bert-serving-client) (19.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bert-serving-client) (1.18.5)\n",
            "Installing collected packages: bert-serving-client\n",
            "Successfully installed bert-serving-client-1.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt4wupKD2uHN"
      },
      "source": [
        "# Start the BERT server\n",
        "bert_command = 'bert-serving-start -model_dir /content/uncased_L-12_H-768_A-12'\n",
        "process = subprocess.Popen(bert_command.split(), stdout=subprocess.PIPE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Fm3uKLf3Au7",
        "outputId": "60d91d99-0b6e-4f96-c325-082b3f4c8f4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "# Start the BERT client\n",
        "from bert_serving.client import BertClient\n",
        "bc = BertClient(check_length=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f51fe95237d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start the BERT client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert_serving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'verbose'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UpYbZjf3Htc"
      },
      "source": [
        "train_df=pd.read_csv('train_2kmZucJ.csv')\n",
        "print(train_df.shape)\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h26hbpJg3TGG"
      },
      "source": [
        "#removing duplicate rows and rows with missing values\n",
        "train_df=train_df.drop_duplicates()\n",
        "train_df=train_df.dropna()\n",
        "print(train_df.shape)\n",
        "\n",
        "#preprocessing\n",
        "import re\n",
        "train_df['tweet']=train_df['tweet'].apply(lambda x:re.sub(r'https?://\\S+', '', x))\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer  \n",
        "lemmatizer = WordNetLemmatizer()  \n",
        "\n",
        "tweets_lst=[]\n",
        "for sent in train_df['tweet'].values:\n",
        "  str_line=\"\"\n",
        "  for word in sent.split(\" \"):\n",
        "    word=re.sub('[^A-Za-z0-9]+', '', word) #removing special characters\n",
        "    if len(word)>0 and len(word)<16: \n",
        "      word=word.lower()  #converting to lower case.\n",
        "      word=lemmatizer.lemmatize(word) #lemmatization\n",
        "      str_line+=word+\" \"\n",
        "  str_line=str_line.strip()\n",
        "  tweets_lst.append(str_line)\n",
        "\n",
        "train_df['tweet']=tweets_lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AW4lBrlH3jMA"
      },
      "source": [
        "# Compute embeddings for training tweets using Bert Client encode function\n",
        "# The model returns 768-dimensional embeddings\n",
        "train_df['tweet'] = train_df['tweet'].apply(lambda x:' '.join(x.split()))\n",
        "tweets = train_df['tweet']\n",
        "\n",
        "tweet_list = [word for word in tweets]\n",
        "embeddings = bc.encode(tweet_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nw8A-qqI3nws"
      },
      "source": [
        "# save bert_train_new for reuse as it would take a really long time for conversion\n",
        "pickle_out = open(\"bert_train.pickle\",\"wb\")\n",
        "pickle.dump(embeddings, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-3-a9wX3tkH"
      },
      "source": [
        "# Creating Test Dataframe\n",
        "test_df=pd.read_csv('test_oJQbWVk.csv')\n",
        "print(test_df.shape)\n",
        "test_df.head()\n",
        "\n",
        "#removing all urls.\n",
        "import re\n",
        "test_df['tweet']=test_df['tweet'].apply(lambda x:re.sub(r'https?://\\S+', '', x))\n",
        "\n",
        "#converting to lower case and removing special characters\n",
        "test_tweets_lst=[]\n",
        "for sent in test_df['tweet'].values:\n",
        "  str_line=\"\"\n",
        "  for word in sent.split(\" \"):\n",
        "    word=re.sub('[^A-Za-z0-9]+', '', word)\n",
        "    if len(word)>1 and len(word)<16:\n",
        "      word=word.lower()\n",
        "      str_line+=word+\" \"\n",
        "  str_line=str_line.strip()\n",
        "  test_tweets_lst.append(str_line)\n",
        "\n",
        "test_df['tweet']=test_tweets_lst\n",
        "test_df['tweet'] = test_df['tweet'].apply(lambda x:' '.join(x.split()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5xsGhSA3-5D"
      },
      "source": [
        "# Compute Bert embeddings for some test tweets\n",
        "test_tweets = test_df['tweet']\n",
        "test_tweet_list = [words for words in test_tweets]\n",
        "test_embeddings = bc.encode(test_tweet_list)\n",
        "test_embeddings.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tEWPyKA4A1G"
      },
      "source": [
        "# save bert_test_new\n",
        "pickle_out = open(\"bert_test.pickle\",\"wb\")\n",
        "pickle.dump(test_embeddings, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}